# main.py
import argparse
from typing import Optional, List, Dict

from config import SEARCH_KEYWORDS
from scraper import fetch_papers
from db_utils import create_tables, upsert_paper, get_connection, get_full_paper
from summarizer import run_summarizer


def _find_existing_paper_id(title: str, pdf_url: Optional[str], source: str = "arXiv") -> Optional[int]:
    """
    Check if a paper already exists by pdf_url or (title, source).
    Returns the paper_id if found, else None.
    """
    conn = get_connection()
    cur = conn.cursor()

    if pdf_url:
        cur.execute("SELECT id FROM papers WHERE pdf_url = ?", (pdf_url,))
        row = cur.fetchone()
        if row:
            conn.close()
            return row[0]

    cur.execute("SELECT id FROM papers WHERE title = ? AND source = ?", (title, source))
    row = cur.fetchone()
    conn.close()
    return row[0] if row else None


def run_fetch_flow():
    """Fetch, insert, and summarize new papers."""
    print("ğŸš€ Initializing database...")
    create_tables()

    print("\nğŸ“¥ Fetching latest papers from arXiv...")
    papers: List[Dict] = fetch_papers(SEARCH_KEYWORDS, max_results=3)
    print(f"Found {len(papers)} candidate papers.")

    new_ids = []
    existing = 0

    for p in papers:
        title = p["title"]
        pdf_url = p.get("pdf_url")
        source = "arXiv"

        preexisting_id = _find_existing_paper_id(title, pdf_url, source)

        paper_id = upsert_paper(
            title=title,
            authors=p.get("authors"),
            abstract=p.get("abstract"),
            pdf_url=pdf_url,
            source=source,
            published_date=p.get("published"),
            keyword=p.get("keyword"),
        )

        if preexisting_id is None:
            new_ids.append(paper_id)
            print(f"âœ… Inserted NEW: {title[:80]} (id={paper_id})")
        else:
            existing += 1
            print(f"â†©ï¸  Already present: {title[:80]} (id={paper_id}) â€” skipping summarization")

    if new_ids:
        print(f"\nğŸ“ Running summarizer for {len(new_ids)} new papers...")
        for pid in new_ids:
            run_summarizer(pid)
    else:
        print("\nğŸ“ No new papers to summarize.")

    print(f"\nğŸ“Š Done. New: {len(new_ids)} | Existing: {existing}")


def run_view_flow(paper_id: int):
    """Display a saved paper with all details in a pretty format."""
    full = get_full_paper(paper_id)

    if not full or not full["paper"]:
        print(f"âŒ No paper found with ID {paper_id}")
        return

    paper = full["paper"]
    summaries = full["summaries"]
    facts = full["facts"]
    entities = full["entities"]
    mindmap_json = full["mindmap_json"]

    print("\nğŸ“„ Paper Details")
    print("=" * 60)
    print(f"ğŸ†” ID: {full['id']}")
    print(f"ğŸ“Œ Title: {paper['title']}")
    print(f"ğŸ‘¨â€ğŸ”¬ Authors: {paper['authors']}")
    print(f"ğŸ”— PDF: {paper['pdf_url']}")

    print("\nğŸ“ Summaries")
    print("-" * 60)
    if summaries:
        print(f"â¡ï¸ Short: {summaries['short']}")
        print(f"ğŸ“– Long: {summaries['long']}")
    else:
        print("âŒ No summaries available.")

    print("\nğŸ“Š Facts")
    print("-" * 60)
    if facts:
        for f in facts:
            print(f"â€¢ {f['fact_type']}: {f['fact_value']}")
    else:
        print("âŒ No facts extracted.")

    print("\nğŸ” Entities")
    print("-" * 60)
    if entities:
        for e in entities:
            print(f"â€¢ {e['entity']} ({e['entity_type']})")
    else:
        print("âŒ No entities found.")

    print("\nğŸ§  Mindmap")
    print("-" * 60)
    if mindmap_json:
        print(mindmap_json)
    else:
        print("âŒ No mindmap available.")



def main():
    parser = argparse.ArgumentParser(
        description="ğŸ“š Agentic AI Research Hub CLI"
    )
    parser.add_argument(
        "--fetch", action="store_true",
        help="Fetch latest papers and summarize new ones"
    )
    parser.add_argument(
        "--view", type=int, metavar="PAPER_ID",
        help="View a paper by its database ID"
    )

    args = parser.parse_args()

    if args.view:
        run_view_flow(args.view)
    elif args.fetch:
        run_fetch_flow()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
